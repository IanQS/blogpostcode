{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is accompanied by my blogpost where I go into more detail about the decisions I've made, and whatever notes I may make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Training Pipeline\n",
    "\n",
    "## 1.1) Producer:\n",
    "\n",
    "- takes `numpy` arrays or `CSVs`\n",
    "\n",
    "- $\\lambda$ `np.ndarray` -> `tf.TfRecords`\n",
    "\n",
    "- loads from `raw_data` folder in this tutorial and writes to `processed_data` folder \n",
    "\n",
    "## 1.2) Provider:\n",
    "\n",
    "- loads from `processed_data` folder \n",
    "\n",
    "- processes the data (so that the processing is part of the computation graph)\n",
    "\n",
    "- loads `tf.TfRecords` and sends it directly to tensorflow. Avoids `feed_dict` which is [reportedly slow](https://www.tensorflow.org/guide/performance/overview#input_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer:\n",
    "\n",
    "## Method 1\n",
    "\n",
    "Cell 1: Load the numpy array\n",
    "\n",
    "Cell 2: Conversion of a row of data into formats compatible with tf\n",
    "\n",
    "Cell 3: Save it as a tfRecords file\n",
    "\n",
    "## Method 2 (not shown here)\n",
    "\n",
    "You can load it in as a tf.data.Dataset, then use the `experimenta` library to construct a tfRecords file. A good resource for this is [official docs](https://www.tensorflow.org/tutorials/load_data/tf-records#tfexample)\n",
    "\n",
    "\n",
    "```\n",
    "serialized_features_dataset = features_dataset.map(tf_serialize_example)\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 55)\n"
     ]
    }
   ],
   "source": [
    "loaded = np.loadtxt('unprocessed_data/covtype.data', delimiter=',')  # Avoid tf.contrib since we want to get our hands dirty\n",
    "print(loaded.shape)\n",
    "all_ind = np.arange(0, len(loaded))\n",
    "train_ind = all_ind[: int(len(loaded) * 0.8)]\n",
    "test_ind = all_ind[int(len(loaded) * 0.8): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse records\n",
    "\n",
    "\"\"\"\n",
    "Elevation                               quantitative    meters                       Elevation in meters\n",
    "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "Slope                                   quantitative    degrees                      Slope in degrees\n",
    "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "\"\"\"\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _tensor_feature(value, kwd, underlying_type):\n",
    "    \"\"\"Returns a tensor feature from a list of values (that were first converted to a tensor)\n",
    "    \n",
    "    # I tried using the following, but tf.train.Feature expects bytes, not a Tensor of bytes (avoid doing this!)\n",
    "    tensor = tf.convert_to_tensor(value)\n",
    "    print(tensor)\n",
    "    serialized = tf.serialize_tensor(tensor)\n",
    "    print(serialized)\n",
    "    print(dir(serialized))\n",
    "    serialized =  tf.compat.bytes_or_text_types(serialized)\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized]))\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(**{\n",
    "        kwd: underlying_type(value=value)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    for record_type in [('train', train_ind), ('test', test_ind)]:\n",
    "\n",
    "        filename = 'processed_data/tf_record_covtype_{}_{}'.format(\n",
    "            record_type[0], datetime.datetime.now().replace(microsecond=0,second=0,minute=0)\n",
    "        )  # Round to the previous hour\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for i in tqdm.tqdm_notebook(record_type[1]):\n",
    "                datum = loaded[i, :]\n",
    "                feature = {\n",
    "                    'Elevation': _float_feature(datum[0]),\n",
    "                    'Aspect': _float_feature(datum[1]),\n",
    "                    'Slope': _float_feature(datum[2]),\n",
    "                    'Horizontal_Distance_To_Hydrology': _float_feature(datum[3]),\n",
    "                    'Vertical_Distance_To_Hydrology': _float_feature(datum[4]),\n",
    "                    'Hillshade_9am': _float_feature(datum[5]),\n",
    "                    'Hillshade_Noon': _float_feature(datum[6]),\n",
    "                    'Hillshade_3pm': _float_feature(datum[7]),\n",
    "                    'Horizontal_Distance_To_Fire_Points': _float_feature(datum[8]),\n",
    "                    'Wilderness_Area': _tensor_feature(datum[9:13], 'float_list', tf.train.FloatList),\n",
    "                    'Soil_Type': _tensor_feature(datum[14:54], 'float_list', tf.train.FloatList),\n",
    "                    'Cover Type': _float_feature(datum[54])\n",
    "                }\n",
    "                example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example_proto.SerializeToString())\n",
    "\n",
    "        # using your storage system -S3 or some other file hosting service, add the export here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider\n",
    "\n",
    "Cell 1: Initialize the loader\n",
    "\n",
    "- even though there is the train, and test data in the tfRecordDataset, we pretend that they're two different runs of our pre-processor\n",
    "\n",
    "Cell 2: Provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(example_proto):\n",
    "    # First, parse the dataset back \n",
    "    features = {\n",
    "        'Elevation': tf.FixedLenFeature((), tf.float32),\n",
    "        'Aspect': tf.FixedLenFeature((), tf.float32),\n",
    "        'Slope': tf.FixedLenFeature((), tf.float32),\n",
    "        'Horizontal_Distance_To_Hydrology': tf.FixedLenFeature((), tf.float32),\n",
    "        'Vertical_Distance_To_Hydrology': tf.FixedLenFeature((), tf.float32),\n",
    "        'Hillshade_9am': tf.FixedLenFeature((), tf.float32),\n",
    "        'Hillshade_Noon': tf.FixedLenFeature((), tf.float32),\n",
    "        'Hillshade_3pm': tf.FixedLenFeature((), tf.float32),\n",
    "        'Horizontal_Distance_To_Fire_Points': tf.FixedLenFeature((), tf.float32),\n",
    "        'Wilderness_Area': tf.FixedLenFeature((4), tf.float32),\n",
    "        'Soil_Type': tf.FixedLenFeature((40), tf.float32),\n",
    "        'Cover Type': tf.FixedLenFeature((), tf.float32)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    labels = parsed_features['Cover Type']\n",
    "    parsed_features.pop('Cover Type')\n",
    "    \n",
    "    # Then, convert the dataset into tensors which tensorflow expects?\n",
    "    \n",
    "    \n",
    "    return parsed_features, labels\n",
    " \n",
    "\n",
    "\n",
    "def dataset_config(filenames: list, mapper=None, repeat=False, batch_size=32,\n",
    "                  initializable=False, sess=None, feed_dict=None, num_cpus=None):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    if mapper is not None:\n",
    "        dataset = dataset.map(mapper, num_parallel_calls=num_cpus)\n",
    "        \n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if initializable:\n",
    "        \"\"\"\n",
    "        An initializable iterator requires you to run an explicit iterator.initializer operation before using it. \n",
    "        In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, \n",
    "        using one or more tf.placeholder() tensors that can be fed when you initialize the iterator\n",
    "        \"\"\"\n",
    "        # Creates an Iterator for enumerating the elements of this dataset\n",
    "        if sess is None:\n",
    "            raise Exception('Initializable dataset configuration specified but session not supplied')\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    else:\n",
    "        \"\"\"\n",
    "        A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, \n",
    "        with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing \n",
    "        queue-based input pipelines support, but they do not support parameterization\n",
    "        \"\"\"\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        \n",
    "    next_element = iterator.get_next()\n",
    "    if initializable:\n",
    "        assert feed_dict is not None, 'Supply feed dict to initializable iterator'\n",
    "        sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "    \n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/tf_record_covtype_train_2018-12-26 18:00:00', 'processed_data/tf_record_covtype_test_2018-12-26 17:00:00', 'processed_data/tf_record_covtype_train_2018-12-26 17:00:00']\n"
     ]
    }
   ],
   "source": [
    "filename_list = []\n",
    "for dirname, dirnames, filenames in os.walk('processed_data/'):\n",
    "    # print path to all subdirectories first.\n",
    "    for f in filenames:\n",
    "        filename_list.append('{}{}'.format(dirname, f))\n",
    "print(filename_list)\n",
    "dataset = tf.data.TFRecordDataset(filename_list)\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "training_dataset = dataset_config(filename_list, mapper=unpack, num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, label = training_dataset.get_next(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
