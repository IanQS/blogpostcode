{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- uses functions from [utils](https://github.com/IanQS/blogpostcode/blob/master/src/Tf_Exploration/exploration/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tf_Exploration.exploration.utils import FeatureProto, dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/covtype_train_1_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_3_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_0_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_2_2019-01-06_02:00:00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "filename_list = []\n",
    "evaluation_list = []\n",
    "for root, dirs, files in os.walk('processed_data/'):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        if \"tfrecord\" in f:\n",
    "            if \"train\" in f:\n",
    "                filename_list.append(path)\n",
    "            elif \"test\" in f:\n",
    "                evaluation_list.append(path)\n",
    "            else:\n",
    "                print('Unmatched: {}'.format(path))\n",
    "        \n",
    "print(filename_list)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filename_list)\n",
    "\n",
    "feature_proto = FeatureProto(one_hot=True)\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "def train_func():\n",
    "    return dataset_config(filenames=filename_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "def test_func():\n",
    "    return dataset_config(filenames=evaluation_list, batch_size=4096, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "columns = feature_proto.get_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder_with_default(True, shape=(), name='Is_Training')\n",
    "features, labels = tf.cond(is_training, train_func, test_func)\n",
    "\n",
    "dense_tensor = tf.feature_column.input_layer(features=features, feature_columns=columns)\n",
    "for units in [256, 16]:\n",
    "    dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\n",
    "logits = tf.layers.dense(dense_tensor, 8)\n",
    "\n",
    "# Verification\n",
    "correct_pred = tf.equal(tf.cast(logits, tf.int32), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Training \n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Iteration: 1: Loss: 517.5015869140625 Accuracy: 0.0\n",
      "Iteration: 1001: Loss: 1.0586555004119873 Accuracy: 0.48236083984375\n",
      "Iteration: 2001: Loss: 1.135718584060669 Accuracy: 0.357940673828125\n",
      "Iteration: 3001: Loss: 1.4994240999221802 Accuracy: 0.359619140625\n",
      "Iteration: 4001: Loss: 1.1727173328399658 Accuracy: 0.360504150390625\n",
      "Iteration: 5001: Loss: 1.1828272342681885 Accuracy: 0.482177734375\n",
      "Iteration: 5701: Loss: 1.139478325843811 Accuracy: 0.482177734375\n",
      "Out of range\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    keep_iterating = True\n",
    "\n",
    "    i = 0\n",
    "    print('Accuracy: {}'.format(sess.run(accuracy)))\n",
    "    while keep_iterating:\n",
    "        i += 1\n",
    "        try:\n",
    "            _, loss_val = sess.run([train_op, loss_op])\n",
    "            if i % 1000 == 1:\n",
    "                accuracy_value = sess.run(accuracy, feed_dict={is_training: False})\n",
    "                print('Iteration: {}: Loss: {} Accuracy: {}'.format(i, loss_val, accuracy_value))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Iteration: {}: Loss: {} Accuracy: {}'.format(i, loss_val, accuracy_value))\n",
    "            print('Out of range')\n",
    "            keep_iterating = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            keep_iterating = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canned Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "feature_proto = FeatureProto(one_hot=False)\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "def input_fn_train(): # returns x, y (where y represents label's class index).\n",
    "    return dataset_config(filenames=filename_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "def input_fn_eval(): # returns x, y (where y represents label's class index).\n",
    "    return dataset_config(filenames=evaluation_list, batch_size=2048, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "\n",
    "def evaluate(estimator):\n",
    "    # Fit model.\n",
    "    fit = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(fit)\n",
    "    estimator.train(input_fn=input_fn_train)\n",
    "    fit = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.30092165, 'average_loss': 263.777, 'loss': 537748.75, 'global_step': 0}\n",
      "{'accuracy': 0.48942798, 'average_loss': 1.2048812, 'loss': 2456.33, 'global_step': 5701}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.estimator import DNNClassifier\n",
    "\n",
    "estimator = DNNClassifier(\n",
    "    feature_columns=columns,\n",
    "    n_classes=8,\n",
    "    hidden_units=[256, 16],\n",
    "    optimizer=lambda: tf.train.AdamOptimizer(\n",
    "        learning_rate=tf.train.exponential_decay(\n",
    "            learning_rate=0.1,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.96)\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.0, 'average_loss': 2.079439, 'loss': 4239.246, 'global_step': 0}\n",
      "{'accuracy': 0.48942798, 'average_loss': 1.2046915, 'loss': 2455.9434, 'global_step': 5701}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.estimator import BaselineClassifier\n",
    "\n",
    "classifier = BaselineClassifier(n_classes=8)\n",
    "\n",
    "evaluate(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
