{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Contains setup functions (inherited from data_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tf_Exploration.exploration.utils import FeatureProto, dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/tf_record_covtype_test_2018-12-27 21:00:00', 'processed_data/tf_record_covtype_train_2018-12-27 21:00:00']\n"
     ]
    }
   ],
   "source": [
    "filename_list = []\n",
    "for dirname, dirnames, filenames in os.walk('processed_data/'):\n",
    "    # print path to all subdirectories first.\n",
    "    for f in filenames:\n",
    "        filename_list.append('{}{}'.format(dirname, f))\n",
    "print(filename_list)\n",
    "dataset = tf.data.TFRecordDataset(filename_list)\n",
    "\n",
    "feature_proto = FeatureProto()\n",
    "num_cpus = os.cpu_count()\n",
    "features, labels = dataset_config(filename_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus,\n",
    "                                 repeat=True)\n",
    "columns = feature_proto.get_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network\n",
    "\n",
    "input_layer takes 2 args:\n",
    "\n",
    "    features: \n",
    "    \n",
    "        result of parsing the dataset (parse_example)\n",
    "        \n",
    "        dictionary\n",
    "\n",
    "    feature_columns: \n",
    "        \n",
    "        series of keys to lookup in the features dict\n",
    "    \n",
    "        list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.35716817, 'average_loss': 223.98567, 'loss': 458233.66, 'global_step': 0}\n",
      "{'accuracy': 0.36460522, 'average_loss': 1.321195, 'loss': 2702.923, 'global_step': 9079}\n"
     ]
    }
   ],
   "source": [
    "dense_tensor = tf.feature_column.input_layer(features=features, feature_columns=columns)\n",
    "use_custom = False  # Vs estimator\n",
    "\n",
    "if use_custom:\n",
    "    inputs = dense_tensor\n",
    "    for units in [256, 16]:\n",
    "        dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\n",
    "    logits = tf.layers.dense(dense_tensor, 8)\n",
    "\n",
    "    # Verification\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Training \n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "else:\n",
    "    from tensorflow.estimator import DNNClassifier\n",
    "    \n",
    "#     estimator = DNNClassifier(\n",
    "#         feature_columns=columns,\n",
    "#         n_classes=8,\n",
    "#         hidden_units=[256, 16, 8], logging_hook=logging_hook)\n",
    "    \n",
    "    estimator = DNNClassifier(\n",
    "        feature_columns=columns,\n",
    "        n_classes=8,\n",
    "        hidden_units=[256, 16, 8],\n",
    "        optimizer=lambda: tf.train.AdamOptimizer(\n",
    "            learning_rate=tf.train.exponential_decay(\n",
    "                learning_rate=0.1,\n",
    "                global_step=tf.train.get_global_step(),\n",
    "                decay_steps=10000,\n",
    "                decay_rate=0.96)\n",
    "        )\n",
    "            \n",
    "    )\n",
    "    \n",
    "    def input_fn_train(): # returns x, y (where y represents label's class index).\n",
    "        return dataset_config(filename_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "    def input_fn_eval(): # returns x, y (where y represents label's class index).\n",
    "        return dataset_config(filename_list, batch_size=2048, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "    # Fit model.\n",
    "    loss = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(loss)\n",
    "    estimator.train(input_fn=input_fn_train)\n",
    "    loss = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(loss)\n",
    "    \n",
    "\n",
    "    # Evaluate cross entropy between the test and train labels.\n",
    "    loss = estimator.evaluate(input_fn=input_fn_eval)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.0, 'average_loss': 2.0794415, 'loss': 4254.1567, 'global_step': 0}\n",
      "{'accuracy': 0.36460522, 'average_loss': 1.3038024, 'loss': 2667.3408, 'global_step': 9079}\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Build BaselineClassifier\n",
    "####################################################\n",
    "\n",
    "from tensorflow.estimator import BaselineClassifier\n",
    "classifier = BaselineClassifier(n_classes=8)\n",
    "\n",
    "# Input builders\n",
    "def input_fn_train(): # returns x, y (where y represents label's class index).\n",
    "    return dataset_config(filename_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "def input_fn_eval(): # returns x, y (where y represents label's class index).\n",
    "    return dataset_config(filename_list, batch_size=2048, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "# Fit model.\n",
    "loss = classifier.evaluate(input_fn=input_fn_eval)\n",
    "print(loss)\n",
    "classifier.train(input_fn=input_fn_train)\n",
    "# Evaluate cross entropy between the test and train labels.\n",
    "loss = classifier.evaluate(input_fn=input_fn_eval)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
