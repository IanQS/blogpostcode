{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- uses functions from [utils](https://github.com/IanQS/blogpostcode/blob/master/src/Tf_Exploration/exploration/utils.py)\n",
    "\n",
    "# Custom Estimator\n",
    "\n",
    "Do yourself a favor and read [Creating Custom Estimators](https://www.tensorflow.org/guide/custom_estimators) which work BEAUTIFULLY with the `tf.data.Datasets`. \n",
    "\n",
    "Yeah, it's not as \"low level\" but if all you care about is defining your custom #leet #complex model, the custom estimator handles all the cruft of getting it from experiment to production\n",
    "\n",
    "But of course, I'll show you how to make one here ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "from Tf_Exploration.exploration.utils import FeatureProto, dataset_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "1) Get the training and testing data\n",
    "\n",
    "2) get the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "['processed_data/covtype_train_1_2019-01-06_02:00:00.tfrecord',\n",
      " 'processed_data/covtype_train_3_2019-01-06_02:00:00.tfrecord',\n",
      " 'processed_data/covtype_train_0_2019-01-06_02:00:00.tfrecord',\n",
      " 'processed_data/covtype_train_2_2019-01-06_02:00:00.tfrecord']\n",
      "**********\n",
      "\n",
      "Evaluation Data\n",
      "['processed_data/covtype_test_2019-01-06_02:00:00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "# Get the training data\n",
    "\n",
    "filename_list = []\n",
    "evaluation_list = []\n",
    "for root, dirs, files in os.walk('processed_data/'):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        if \"tfrecord\" in f:\n",
    "            if \"train\" in f:\n",
    "                filename_list.append(path)\n",
    "            elif \"test\" in f:\n",
    "                evaluation_list.append(path)\n",
    "            else:\n",
    "                print('Unmatched: {}'.format(path))\n",
    "        \n",
    "print(\"Training Data\")\n",
    "pprint.pprint(filename_list)\n",
    "print('*' * 10)\n",
    "print(\"\\nEvaluation Data\")\n",
    "pprint.pprint(evaluation_list)\n",
    "\n",
    "\n",
    "# Instantiate our FeatureProto from Tutorial 1: https://github.com/IanQS/blogpostcode/blob/master/src/Tf_Exploration/exploration/Notebook1-data_exploration.ipynb\n",
    "# to get the columns\n",
    "\n",
    "feature_proto = FeatureProto(one_hot=False)\n",
    "columns = feature_proto.get_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canned Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def wrap_training_data(tf_record_train_list, feature_proto, num_cpus):\n",
    "    def input_fn_train(): # returns x, y (where y represents label's class index).\n",
    "        return dataset_config(filenames=tf_record_train_list, batch_size=64, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "    return input_fn_train\n",
    "\n",
    "def wrap_testing_data(tf_record_test_list, feature_proto, num_cpus):\n",
    "    def input_fn_eval(): # returns x, y (where y represents label's class index).\n",
    "        return dataset_config(filenames=tf_record_test_list, batch_size=2048, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "    return input_fn_eval\n",
    "\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "input_fn_train = wrap_training_data(filename_list, feature_proto, num_cpus)\n",
    "input_fn_eval = wrap_testing_data(evaluation_list, feature_proto, num_cpus)\n",
    "\n",
    "def evaluate(estimator):\n",
    "    # Fit model.\n",
    "    fit = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(fit)\n",
    "    estimator.train(input_fn=input_fn_train)\n",
    "    fit = estimator.evaluate(input_fn=input_fn_eval)\n",
    "    print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.estimator import DNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.48739704, 'average_loss': 223.28908, 'loss': 455208.06, 'global_step': 0}\n",
      "{'accuracy': 0.48942798, 'average_loss': 1.211182, 'loss': 2469.175, 'global_step': 5701}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "estimator = DNNClassifier(\n",
    "    feature_columns=columns,\n",
    "    model_dir='save_dir/canned_fuk/',\n",
    "    n_classes=8,\n",
    "    hidden_units=[256, 16],\n",
    "    optimizer=lambda: tf.train.AdamOptimizer(\n",
    "        learning_rate=tf.train.exponential_decay(\n",
    "            learning_rate=0.1,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.96)\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.0, 'average_loss': 2.079439, 'loss': 4239.246, 'global_step': 0}\n",
      "{'accuracy': 0.48942798, 'average_loss': 1.2046517, 'loss': 2455.862, 'global_step': 5701}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.estimator import BaselineClassifier\n",
    "\n",
    "classifier = BaselineClassifier(n_classes=8, model_dir='save_dir/baseline')\n",
    "\n",
    "evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_accuracy': 0.32595545, 'loss': 132.07243, 'global_step': 0}\n",
      "{'custom_accuracy': 0.48942798, 'loss': 1.2051108, 'global_step': 5701}\n"
     ]
    }
   ],
   "source": [
    "def model_definition(features, feature_columns, labels):\n",
    "    \"\"\"\n",
    "    Implementation of your #leet model\n",
    "    \n",
    "    Params:\n",
    "        input_layer: tf.feature_column.input_layer\n",
    "    \n",
    "    returns logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your network\n",
    "    input_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "    layer_1 = tf.layers.Dense(256, activation=tf.nn.relu)(input_layer)\n",
    "    layer_2 = tf.layers.Dense(16, activation=tf.nn.relu)(layer_1)\n",
    "    logits = tf.layers.Dense(8)(layer_2)\n",
    "    \n",
    "    \n",
    "    # Define your prediction, loss, accuracy, and train_op\n",
    "    predictions = {'Class_ID': tf.argmax(input=logits, axis=1)}\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    accuracy = tf.metrics.accuracy(labels, predictions['Class_ID'])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.01)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return predictions, loss, accuracy, train_op\n",
    "    \n",
    "\n",
    "def model_wrapper(feature_proto, abstract_model):\n",
    "    def custom_model(features, labels, mode):\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "        \n",
    "        predictions, loss, accuracy, train_op = model_definition(\n",
    "            features, feature_proto.get_feature_columns(), labels\n",
    "        )\n",
    "\n",
    "        # Prediction\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, \n",
    "                loss=loss, \n",
    "                eval_metric_ops={'custom_accuracy': accuracy}\n",
    "            )\n",
    "        \n",
    "        # Track the accuracy while in training mode\n",
    "        tf.summary.scalar('my_accuracy', accuracy[1])\n",
    "        return tf.estimator.EstimatorSpec( mode, loss=loss, train_op=train_op)\n",
    "    return custom_model\n",
    "\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=model_wrapper(feature_proto, model_definition),\n",
    "    model_dir='save_dir/custom/'\n",
    ")\n",
    "\n",
    "evaluate(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
