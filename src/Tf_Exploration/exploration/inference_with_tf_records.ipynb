{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Contains setup functions (inherited from data_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation\n",
    "# Dataset reading\n",
    "# placeholder definition\n",
    "\n",
    "class FeatureProto(object):\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    proto = namedtuple('prototype', ['name', 'dtype', 'shape'])\n",
    "    \n",
    "    features = [\n",
    "        proto(name='Elevation', dtype=tf.float32, shape=1),\n",
    "        proto(name='Aspect', dtype=tf.float32, shape=1),\n",
    "        proto(name='Slope', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Vertical_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Roadways', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_9am', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_Noon', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_3pm', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Fire_Points', dtype=tf.float32, shape=1),\n",
    "        proto(name='Wilderness_Area', dtype=tf.float32, shape=4),\n",
    "        proto(name='Soil_Type', dtype=tf.float32, shape=40),\n",
    "        proto(name='Cover_Type', dtype=tf.float32, shape=1),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    def dataset_creation(self, data):\n",
    "        idx = 0\n",
    "        collection = {}\n",
    "        for prototype in self.features:\n",
    "            feat_name = prototype.name\n",
    "            dtype = prototype.dtype\n",
    "            shape = prototype.shape\n",
    "            \n",
    "            if dtype == tf.float32:\n",
    "                if shape == 1:\n",
    "                    datum = data[idx]\n",
    "                    encoded_feature = _float_feature(datum)\n",
    "                else:\n",
    "                    datum = data[idx: idx+shape]\n",
    "                    encoded_feature = _tensor_feature(datum, 'float_list', tf.train.FloatList)\n",
    "            else:\n",
    "                raise NotImplementedError('dataset creation for non-float32 not supported')\n",
    "            \n",
    "            collection[feat_name] = encoded_feature\n",
    "            idx += shape\n",
    "        return collection\n",
    "    \n",
    "    def dataset_parsing(self):\n",
    "        if hasattr(self, 'parser_proto'):\n",
    "            return self.parser_proto\n",
    "        else:\n",
    "            parser_proto = {}\n",
    "            for prototype in self.features:\n",
    "                feat_name = prototype.name\n",
    "                dtype = prototype.dtype\n",
    "                shape = prototype.shape\n",
    "                parser_proto[feat_name] = tf.FixedLenFeature(() if shape == 1 else (shape), dtype)\n",
    "            self.parser_proto = parser_proto\n",
    "            return self.parser_proto\n",
    "\n",
    "#     def placeholder_creation():\n",
    "#         \"\"\"\n",
    "#         Used Temporarily because \n",
    "#         \"\"\"\n",
    "#         parser_proto = {}\n",
    "#         for prototype in self.features:\n",
    "#             feat_name = prototype.name\n",
    "#             dtype = prototype.dtype\n",
    "#             shape = prototype.shape\n",
    "#             parser_proto[feat_name] = tf.FixedLenFeature(() if shape == 1 else (shape), dtype)\n",
    "\n",
    "feature_proto = FeatureProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proto_wrap(feature_proto):\n",
    "    features = feature_proto.dataset_parsing()\n",
    "    def unpack(example_proto):\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        labels = parsed_features['Cover_Type']\n",
    "        parsed_features.pop('Cover_Type')\n",
    "        # Then, convert the dataset into tensors which tensorflow expects?\n",
    "        parsed_features['Soil_Type'] = tf.convert_to_tensor(parsed_features['Soil_Type'])\n",
    "        parsed_features['Wilderness_Area'] = tf.cast(tf.argmax(parsed_features['Wilderness_Area'], axis=0), dtype=tf.float32)\n",
    "        labels = tf.one_hot(tf.cast(labels, dtype=tf.uint8), 8, on_value=1, off_value=0, axis=-1)\n",
    "\n",
    "        return parsed_features, labels\n",
    "    return unpack\n",
    " \n",
    "\n",
    "\n",
    "def dataset_config(filenames: list, mapper=None, repeat=False, batch_size=32,\n",
    "                  initializable=False, sess=None, feed_dict=None, num_cpus=None):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    if mapper is not None:\n",
    "        dataset = dataset.map(mapper, num_parallel_calls=num_cpus)\n",
    "        \n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if initializable:\n",
    "        \"\"\"\n",
    "        An initializable iterator requires you to run an explicit iterator.initializer operation before using it. \n",
    "        In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, \n",
    "        using one or more tf.placeholder() tensors that can be fed when you initialize the iterator\n",
    "        \"\"\"\n",
    "        # Creates an Iterator for enumerating the elements of this dataset\n",
    "        if sess is None:\n",
    "            raise Exception('Initializable dataset configuration specified but session not supplied')\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    else:\n",
    "        \"\"\"\n",
    "        A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, \n",
    "        with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing \n",
    "        queue-based input pipelines support, but they do not support parameterization\n",
    "        \"\"\"\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        \n",
    "    if initializable:\n",
    "        assert feed_dict is not None, 'Supply feed dict to initializable iterator'\n",
    "        sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "    \n",
    "    next_element = iterator.get_next()\n",
    "    return next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/tf_record_covtype_test_2018-12-27 21:00:00', 'processed_data/tf_record_covtype_train_2018-12-27 21:00:00']\n"
     ]
    }
   ],
   "source": [
    "filename_list = []\n",
    "for dirname, dirnames, filenames in os.walk('processed_data/'):\n",
    "    # print path to all subdirectories first.\n",
    "    for f in filenames:\n",
    "        filename_list.append('{}{}'.format(dirname, f))\n",
    "print(filename_list)\n",
    "dataset = tf.data.TFRecordDataset(filename_list)\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "contextual_unpacker = proto_wrap(feature_proto)\n",
    "training_dataset_next = dataset_config(filename_list, mapper=contextual_unpacker, num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Code\n",
    "\n",
    "features: A mapping from key to tensors. _FeatureColumns look up via these keys. For example numeric_column('price') will look at 'price' key in this dict. Values can be a SparseTensor or a Tensor depends on corresponding _FeatureColumn.\n",
    "    \n",
    "feature_columns: An iterable containing the FeatureColumns to use as inputs to your model. All items should be instances of classes derived from _DenseColumn such as numeric_column, embedding_column, bucketized_column, indicator_column. If you have categorical features, you can wrap them with an embedding_column or indicator_column\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "price = numeric_column('price')\n",
    "keywords_embedded = embedding_column(\n",
    "    categorical_column_with_hash_bucket(\"keywords\", 10K), dimensions=16)\n",
    "columns = [price, keywords_embedded, ...]\n",
    "features = tf.parse_example(..., features=make_parse_example_spec(columns))\n",
    "dense_tensor = input_layer(features, columns)\n",
    "for units in [128, 64, 32]:\n",
    "  dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\n",
    "prediction = tf.layers.dense(dense_tensor, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-49a8c8cfc5b2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-49a8c8cfc5b2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    columns = [tf.feature_column.numeric_column(feat) for feat in ]\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "columns = [tf.feature_column.numeric_column(feat) for feat in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_tensor = input_layer(features, columns)\n",
    "for units in [256, 16, 8]:\n",
    "    dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\n",
    "prediction = tf.layers.dense(dense_tensor, 1, tf.nn.softmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
