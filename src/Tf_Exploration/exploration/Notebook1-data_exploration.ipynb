{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is accompanied by my blogpost where I go into more detail about the decisions I've made, and whatever notes I may make\n",
    "\n",
    "The link to the [blogpost](https://ianqs.github.io/blog/2019/01/05/TF-dataset-madness)\n",
    "\n",
    "The dataset is from [UCI Covertype dataset](https://archive.ics.uci.edu/ml/datasets/covertype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Training Pipeline\n",
    "\n",
    "## 1.1) Producer:\n",
    "\n",
    "- Ideally takes arbitrary datasets (np, csv, .data)\n",
    "\n",
    "- $\\lambda$: (x) -> `tf.TfRecords`\n",
    "\n",
    "- loads from `unprocessed_data` folder in this tutorial and writes to `processed_data` folder \n",
    "\n",
    "## 1.2) Provider:\n",
    "\n",
    "- loads from `processed_data` folder \n",
    "\n",
    "- processes the data (so that the processing is part of the computation graph)\n",
    "\n",
    "- loads `tf.TfRecords` and sends it directly to tensorflow. Avoids `feed_dict` which is [slow](https://www.tensorflow.org/guide/performance/overview#input_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer:\n",
    "\n",
    "## Method 1\n",
    "\n",
    "1: Load the data\n",
    "\n",
    "2: Conversion of a row of data into formats compatible with tf\n",
    "\n",
    "3: Save it as a tfRecords file\n",
    "\n",
    "## Method 2 (not shown here)\n",
    "\n",
    "You can load it in as a tf.data.Dataset, then use the `experimental` library to construct a tfRecords file. A good resource for this is [official docs](https://www.tensorflow.org/tutorials/load_data/tf-records#tfexample)\n",
    "\n",
    "\n",
    "```\n",
    "serialized_features_dataset = features_dataset.map(tf_serialize_example)\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information\n",
    "\n",
    "    Elevation                               quantitative    meters                       Elevation in meters\n",
    "    \n",
    "    Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "    \n",
    "    Slope                                   quantitative    degrees                      Slope in degrees\n",
    "    \n",
    "    Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "    \n",
    "    Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "    \n",
    "    Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "    \n",
    "    Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "    \n",
    "    Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "    \n",
    "    Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "    \n",
    "    Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "    \n",
    "    Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "    \n",
    "    Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "    \n",
    "    Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer Pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype class - enables easier feature definition and better for refactoring\n",
    "\n",
    "class FeatureProto(object):\n",
    "    from collections import namedtuple\n",
    "    import numpy as np\n",
    "    \n",
    "    proto = namedtuple('prototype', ['name', 'dtype', 'shape'])\n",
    "    \n",
    "    # Reading the data\n",
    "    features = [\n",
    "        proto(name='Elevation', dtype=tf.float32, shape=1),\n",
    "        proto(name='Aspect', dtype=tf.float32, shape=1),\n",
    "        proto(name='Slope', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Vertical_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Roadways', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_9am', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_Noon', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_3pm', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Fire_Points', dtype=tf.float32, shape=1),\n",
    "        proto(name='Wilderness_Area', dtype=tf.float32, shape=4),\n",
    "        proto(name='Soil_Type', dtype=tf.float32, shape=40),\n",
    "        proto(name='Cover_Type', dtype=tf.float32, shape=1),\n",
    "    ]\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        size = 0\n",
    "        for prototype in self.features:\n",
    "            size += prototype.shape\n",
    "        return size\n",
    "    \n",
    "    def dataset_creation(self, data):\n",
    "        idx = 0\n",
    "        collection = {}\n",
    "        for prototype in self.features:            \n",
    "            encoded_feature = self._generate_feature(\n",
    "                prototype.dtype, prototype.shape, data, idx\n",
    "            )\n",
    "            collection[prototype.name] = encoded_feature\n",
    "            idx += prototype.shape\n",
    "        return collection\n",
    "    \n",
    "    def _generate_feature(self, dtype, shape, data, idx):\n",
    "        datum = [data[idx]] if shape == 1 else data[idx:idx + shape]\n",
    "        \n",
    "        if dtype == tf.float16 or dtype == tf.float32 or dtype == tf.float64:\n",
    "            encoded_feature = _float_feature(datum, shape)\n",
    "        elif dtype == tf.int16 or dtype == tf.int32 or dtype == tf.int64:\n",
    "            encoded_feature = _int64_feature(datum, shape)\n",
    "        elif dtype == tf.string:\n",
    "            encoded_feature = _bytes_feature(datum, shape)\n",
    "        else:\n",
    "            raise NotImplementedError('Unmated type while generating feature in FeatureProto')\n",
    "        return encoded_feature\n",
    "    \n",
    "    def unpack(self, example_proto):\n",
    "        features = self._dataset_parsing()\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        labels = parsed_features['Cover_Type']\n",
    "        parsed_features.pop('Cover_Type')\n",
    "        # Then, convert the dataset into tensors which tensorflow expects?\n",
    "        parsed_features['Soil_Type'] = tf.convert_to_tensor(parsed_features['Soil_Type'])\n",
    "        parsed_features['Wilderness_Area'] = tf.cast(tf.argmax(parsed_features['Wilderness_Area'], axis=0), dtype=tf.float32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        #labels = tf.one_hot(tf.cast(labels, dtype=tf.uint8), 8, on_value=1, off_value=0, axis=-1)\n",
    "        return parsed_features, labels\n",
    "            \n",
    "        \n",
    "    def _dataset_parsing(self):\n",
    "        if hasattr(self, 'parser_proto'):\n",
    "            return self.parser_proto\n",
    "        else:\n",
    "            parser_proto = {}\n",
    "            for prototype in self.features:\n",
    "                feat_name = prototype.name\n",
    "                dtype = prototype.dtype\n",
    "                shape = prototype.shape\n",
    "                parser_proto[feat_name] = tf.FixedLenFeature(() if shape == 1 else (shape), dtype)\n",
    "            self.parser_proto = parser_proto\n",
    "            return self.parser_proto\n",
    "\n",
    "\n",
    "feature_proto = FeatureProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip load_data_run load in data from unprocessed_data folder\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    loaded = np.loadtxt('unprocessed_data/covtype.data', delimiter=',', dtype=np.int)  # Avoid tf.contrib since we want to get our hands dirty\n",
    "    print(loaded.shape)\n",
    "    all_ind = np.arange(0, len(loaded))\n",
    "    train_ind = all_ind[: int(len(loaded) * 0.8)]\n",
    "    test_ind = all_ind[int(len(loaded) * 0.8): ]\n",
    "    \n",
    "    return loaded, all_ind, train_ind, test_ind\n",
    "\n",
    "\n",
    "load_data_run = False\n",
    "if load_data_run:\n",
    "    loaded, all_ind, train_ind, test_ind = load_data()\n",
    "else:\n",
    "    print('Flip load_data_run load in data from unprocessed_data folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip generate_run to generate_samples\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(feature_proto):\n",
    "    try:\n",
    "        os.mkdir('processed_data')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    time = str(datetime.datetime.now().replace(microsecond=0,second=0,minute=0)).replace(' ', '_')\n",
    "    for record_type in [('train', train_ind), ('test', test_ind)]:\n",
    "        filename = 'processed_data/tf_record_covtype_{}_{}.tfrecord'.format(\n",
    "            record_type[0],\n",
    "            time\n",
    "        )  # Round to the previous hour\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for i in tqdm.tqdm_notebook(record_type[1]):\n",
    "                datum = loaded[i, :]\n",
    "                feature = feature_proto.dataset_creation(datum)\n",
    "                example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example_proto.SerializeToString())\n",
    "\n",
    "        # using your storage system -S3 or some other file hosting service, add the export here\n",
    "        \n",
    "generate_run = False\n",
    "\n",
    "if generate_run:\n",
    "    generate_samples(feature_proto)\n",
    "else:\n",
    "    print('Flip generate_run to generate_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider\n",
    "\n",
    "Cell 1: Initialize the loader\n",
    "\n",
    "- even though there is the train, and test data in the tfRecordDataset, we pretend that they're two different runs of our pre-processor\n",
    "\n",
    "- The proto_wrap function is unnecessary here but for the sake of clarity I left it in. In the next tutorial, where I show you how to use the loaded data, we will remove it\n",
    "\n",
    "Cell 2: Provide\n",
    "\n",
    "- return an iterator that you can go through to iterate your dataset using either 1) stored tfrecords, or via tf.data.Datasets on numpy/ csvs\n",
    "\n",
    "Cell 3: Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/tf_record_covtype_train_2019-01-05_19:00:00', 'processed_data/tf_record_covtype_test_2019-01-05_19:00:00']\n"
     ]
    }
   ],
   "source": [
    "configuration = 'csv'  # Options: tf, csv, np\n",
    "\n",
    "# Option 1: reading tf.data.TFRecordDataset\n",
    "#     - requires that you generate it first\n",
    "if configuration == 'tf':\n",
    "    filename_list = []\n",
    "    for dirname, dirnames, filenames in os.walk('processed_data/'):\n",
    "        for f in filenames:\n",
    "            if \"tfrecords\" in f:\n",
    "                filename_list.append('{}{}'.format(dirname, f))\n",
    "    print(filename_list)\n",
    "    dataset = tf.data.TFRecordDataset(filename_list)\n",
    "    num_cpus = os.cpu_count()\n",
    "    training_dataset_next = dataset_config(filename_list, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "    \n",
    "# Note, \n",
    "# Option 2: reading as a CSV\n",
    "elif configuration == 'csv':\n",
    "    filename_queue = tf.train.string_input_producer(['unprocessed_data/covtype.csv'])\n",
    "    reader = tf.TextLineReader()\n",
    "    k, v = reader.read(filename_queue)\n",
    "    \n",
    "    record_defaults = [[0] for _ in range(feature_proto.size)]\n",
    "    \n",
    "    columns = tf.decode_csv(v, record_defaults=record_defaults)\n",
    "    \"\"\" FILL IN \"\"\"\n",
    "    \n",
    "\n",
    "# Option 3: Reading Np. Note that there is a 2GB limit and you should avoid this\n",
    "else:\n",
    "    with np.load('unprocessed_data/covtype.npy') as data:\n",
    "        features = data[\"features\"]\n",
    "        labels = data[\"labels\"]\n",
    "        \n",
    "        \n",
    "        features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "        labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "        \n",
    "        training_dataset_next = dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-d3ae370a54e8>:7: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
      "dict_keys(['Aspect', 'Elevation', 'Hillshade_3pm', 'Hillshade_9am', 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Slope', 'Soil_Type', 'Vertical_Distance_To_Hydrology', 'Wilderness_Area'])\n",
      "dict_keys(['Aspect', 'Elevation', 'Hillshade_3pm', 'Hillshade_9am', 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Slope', 'Soil_Type', 'Vertical_Distance_To_Hydrology', 'Wilderness_Area'])\n"
     ]
    }
   ],
   "source": [
    "# Lazy execution\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    if configuration == 'np':\n",
    "\n",
    "    sess.run(init)\n",
    "    for i in range(2):\n",
    "        features, label = sess.run(training_dataset_next)\n",
    "        pprint.pprint(features.keys())\n",
    "    \n",
    "# Eager execution\n",
    "#features, label = training_dataset_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect': array([ 51.,  56., 139., 155.,  45., 132.,  45.,  49.,  45.,  59., 201.,\n",
      "       151., 134., 214., 157.,  51., 259.,  72.,   0.,  38.,  71., 209.,\n",
      "       114.,  54.,  22., 135., 163., 148., 135., 117., 122., 105.],\n",
      "      dtype=float32),\n",
      " 'Elevation': array([2596., 2590., 2804., 2785., 2595., 2579., 2606., 2605., 2617.,\n",
      "       2612., 2612., 2886., 2742., 2609., 2503., 2495., 2610., 2517.,\n",
      "       2504., 2503., 2501., 2880., 2768., 2511., 2507., 2492., 2489.,\n",
      "       2962., 2811., 2739., 2703., 2522.], dtype=float32),\n",
      " 'Hillshade_3pm': array([148., 151., 135., 122., 150., 140., 138., 144., 133., 124., 161.,\n",
      "       136.,  92., 170., 151., 137., 161., 133., 156., 144., 126., 179.,\n",
      "        71., 130., 143., 142., 145., 120., 154.,  71.,  52., 130.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_9am': array([221., 220., 234., 238., 220., 230., 222., 222., 223., 228., 218.,\n",
      "       234., 248., 213., 224., 224., 216., 228., 214., 220., 230., 206.,\n",
      "       252., 225., 215., 229., 230., 240., 220., 253., 254., 233.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_Noon': array([232., 235., 238., 238., 234., 237., 225., 230., 221., 219., 243.,\n",
      "       240., 224., 247., 240., 225., 239., 227., 232., 228., 223., 253.,\n",
      "       209., 222., 221., 237., 243., 236., 238., 210., 201., 231.],\n",
      "      dtype=float32),\n",
      " 'Horizontal_Distance_To_Fire_Points': array([6279., 6225., 6121., 6211., 6172., 6031., 6256., 6228., 6244.,\n",
      "       6230., 6222., 4051., 6091., 6211., 5600., 5576., 6096., 5607.,\n",
      "       5572., 5555., 5547., 4323., 5972., 5569., 5534., 5494., 5486.,\n",
      "       3395., 5643., 6033., 6123., 5569.], dtype=float32),\n",
      " 'Horizontal_Distance_To_Hydrology': array([258., 212., 268., 242., 153., 300., 270., 234., 240., 247., 180.,\n",
      "       371., 150., 150.,  67.,  42., 120.,  85.,  95.,  85.,  60., 216.,\n",
      "       192., 124., 120.,   0.,  30., 323., 212., 127.,  67., 120.],\n",
      "      dtype=float32),\n",
      " 'Horizontal_Distance_To_Roadways': array([ 510.,  390., 3180., 3090.,  391.,   67.,  633.,  573.,  666.,\n",
      "        636.,  735., 5253., 3215.,  771.,  674.,  752.,  607.,  595.,\n",
      "        691.,  741.,  767., 4986., 3339.,  638.,  732.,  860.,  849.,\n",
      "       5916., 3670., 3281., 3191.,  595.], dtype=float32),\n",
      " 'Slope': array([ 3.,  2.,  9., 18.,  2.,  6.,  7.,  4.,  9., 10.,  4., 11., 22.,\n",
      "        7.,  4.,  7.,  1.,  7.,  4.,  5.,  9., 17., 23.,  8.,  9.,  6.,\n",
      "       10., 16.,  1., 24., 30.,  7.], dtype=float32),\n",
      " 'Soil_Type': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
      " 'Vertical_Distance_To_Hydrology': array([  0.,  -6.,  65., 118.,  -1., -15.,   5.,   7.,  56.,  11.,  51.,\n",
      "        26.,  69.,  46.,   4.,   2.,  -1.,   6.,   5.,  10.,   8.,  30.,\n",
      "        82.,   0.,  14.,   0.,  -4.,  23.,  30.,  53.,  27.,   1.],\n",
      "      dtype=float32),\n",
      " 'Wilderness_Area': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done! \n",
    "\n",
    "And with that, we're done! We've \n",
    "\n",
    "1) taken a non-trivial dataset, \n",
    "2) converted it into a `tfRecord`\n",
    "3) shown how to unload it and read from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
