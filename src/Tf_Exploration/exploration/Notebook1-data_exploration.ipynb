{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is accompanied by my blogpost where I go into more detail about the decisions I've made, and whatever notes I may make\n",
    "\n",
    "The link to the [blogpost](https://ianqs.github.io/blog/2019/01/05/TF-dataset-madness)\n",
    "\n",
    "The dataset is from [UCI Covertype dataset](https://archive.ics.uci.edu/ml/datasets/covertype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import sys\n",
    "import pprint\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Training Pipeline\n",
    "\n",
    "## 1.1) Producer:\n",
    "\n",
    "- Ideally takes arbitrary datasets (np, csv, .data)\n",
    "\n",
    "- $\\lambda$: (x) -> `tf.TfRecords`\n",
    "\n",
    "- loads from `unprocessed_data` folder in this tutorial and writes to `processed_data` folder \n",
    "\n",
    "## 1.2) Provider:\n",
    "\n",
    "- loads from `processed_data` folder \n",
    "\n",
    "- processes the data (so that the processing is part of the computation graph)\n",
    "\n",
    "- loads `tf.TfRecords` and sends it directly to tensorflow. Avoids `feed_dict` which is [slow](https://www.tensorflow.org/guide/performance/overview#input_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer:\n",
    "\n",
    "## Method 1\n",
    "\n",
    "1: Load the data\n",
    "\n",
    "2: Conversion of a row of data into formats compatible with tf\n",
    "\n",
    "3: Save it as a tfRecords file\n",
    "\n",
    "## Method 2 (not shown here)\n",
    "\n",
    "You can load it in as a tf.data.Dataset, then use the `experimental` library to construct a tfRecords file. A good resource for this is [official docs](https://www.tensorflow.org/tutorials/load_data/tf-records#tfexample)\n",
    "\n",
    "\n",
    "```\n",
    "serialized_features_dataset = features_dataset.map(tf_serialize_example)\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information\n",
    "\n",
    "    Elevation                               quantitative    meters                       Elevation in meters\n",
    "    \n",
    "    Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "    \n",
    "    Slope                                   quantitative    degrees                      Slope in degrees\n",
    "    \n",
    "    Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "    \n",
    "    Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "    \n",
    "    Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "    \n",
    "    Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "    \n",
    "    Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "    \n",
    "    Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "    \n",
    "    Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "    \n",
    "    Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "    \n",
    "    Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "    \n",
    "    Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer Pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "# Prototype class - enables easier feature definition and better for refactoring\n",
    "\n",
    "class FeatureProto(object):\n",
    "    from collections import namedtuple\n",
    "    import numpy as np\n",
    "    \n",
    "    proto = namedtuple('prototype', ['name', 'dtype', 'shape'])\n",
    "    \n",
    "    # Ordering the features for parsing\n",
    "    features = [\n",
    "        proto(name='Elevation', dtype=tf.float32, shape=1),\n",
    "        proto(name='Aspect', dtype=tf.float32, shape=1),\n",
    "        proto(name='Slope', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Vertical_Distance_To_Hydrology', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Roadways', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_9am', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_Noon', dtype=tf.float32, shape=1),\n",
    "        proto(name='Hillshade_3pm', dtype=tf.float32, shape=1),\n",
    "        proto(name='Horizontal_Distance_To_Fire_Points', dtype=tf.float32, shape=1),\n",
    "        proto(name='Wilderness_Area', dtype=tf.float32, shape=4),\n",
    "        proto(name='Soil_Type', dtype=tf.float32, shape=40),\n",
    "        proto(name='Cover_Type', dtype=tf.float32, shape=1),\n",
    "    ]\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        size = 0\n",
    "        for prototype in self.features:\n",
    "            size += prototype.shape\n",
    "        return size\n",
    "    \n",
    "    \n",
    "    def dataset_creation(self, data):\n",
    "        \"\"\" Used in the producer assuming you're creating tfRecords\"\"\"\n",
    "        idx = 0\n",
    "        collection = {}\n",
    "        for prototype in self.features:\n",
    "            datum = [data[idx]] if prototype.shape == 1 else data[idx:idx + prototype.shape]\n",
    "            encoded_feature = self._generate_feature(\n",
    "                prototype.dtype, datum, idx\n",
    "            )\n",
    "            collection[prototype.name] = encoded_feature\n",
    "            idx += prototype.shape\n",
    "        return collection\n",
    "    \n",
    "    def _generate_feature(self, dtype, data, idx):\n",
    "        \"\"\"Used in tandem with dataset_creation to define the features to write to the dataset\"\"\"\n",
    "        if dtype == tf.float16 or dtype == tf.float32 or dtype == tf.float64:\n",
    "            encoded_feature = _float_feature(data)\n",
    "        elif dtype == tf.int16 or dtype == tf.int32 or dtype == tf.int64:\n",
    "            encoded_feature = _int64_feature(data)\n",
    "        elif dtype == tf.string:\n",
    "            encoded_feature = _bytes_feature(data)\n",
    "        else:\n",
    "            raise NotImplementedError('Unmated type while generating feature in FeatureProto')\n",
    "        return encoded_feature\n",
    "    \n",
    "    def unpack(self, example_proto):\n",
    "        \"\"\"Used in dataset.map where we unpack the dataset as we read it. This is for the tfrecrd\"\"\"\n",
    "        features = self._dataset_parsing()\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        labels = parsed_features['Cover_Type']\n",
    "        parsed_features.pop('Cover_Type')\n",
    "        # Then, convert the dataset into tensors which tensorflow expects?\n",
    "        parsed_features['Soil_Type'] = tf.convert_to_tensor(parsed_features['Soil_Type'])\n",
    "        parsed_features['Wilderness_Area'] = tf.cast(tf.argmax(parsed_features['Wilderness_Area'], axis=0), dtype=tf.float32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        #labels = tf.one_hot(tf.cast(labels, dtype=tf.uint8), 8, on_value=1, off_value=0, axis=-1)\n",
    "        return parsed_features, labels\n",
    "            \n",
    "        \n",
    "    def unpack_csv(self, example_proto):\n",
    "        \"\"\"Used in dataset.map where we unpack the dataset as we read it. This is for the csv data\"\"\"\n",
    "        # 1) Convert the data from bytestring into integers\n",
    "        vals = tf.string_split([example_proto], delimiter=',').values\n",
    "        vals_cvt = tf.string_to_number(vals, out_type=tf.int32)\n",
    "        \n",
    "        parsed_features = {}\n",
    "        idx = 0\n",
    "        for prototype in self.features:\n",
    "            datum = vals_cvt[idx] if prototype.shape == 1 else vals_cvt[idx:idx + prototype.shape]\n",
    "            parsed_features[prototype.name] = datum\n",
    "            idx += prototype.shape\n",
    "            \n",
    "        labels = parsed_features['Cover_Type']\n",
    "        parsed_features.pop('Cover_Type')\n",
    "        # Then, convert the dataset into tensors which tensorflow expects?\n",
    "        parsed_features['Soil_Type'] = tf.convert_to_tensor(parsed_features['Soil_Type'])\n",
    "        parsed_features['Wilderness_Area'] = tf.cast(tf.argmax(parsed_features['Wilderness_Area'], axis=0), dtype=tf.float32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        \n",
    "        return parsed_features, labels\n",
    "        \n",
    "    def _dataset_parsing(self):\n",
    "        \"\"\"Used in unpack to group the values into tf.FixedLenFeatures\"\"\"\n",
    "        if hasattr(self, 'parser_proto'):\n",
    "            return self.parser_proto\n",
    "        else:\n",
    "            parser_proto = {}\n",
    "            for prototype in self.features:\n",
    "                feat_name = prototype.name\n",
    "                dtype = prototype.dtype\n",
    "                shape = prototype.shape\n",
    "                parser_proto[feat_name] = tf.FixedLenFeature(() if shape == 1 else (shape), dtype)\n",
    "            self.parser_proto = parser_proto\n",
    "            return self.parser_proto\n",
    "\n",
    "\n",
    "feature_proto = FeatureProto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to write and then write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip load_data_run load in data from unprocessed_data folder\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    loaded = np.loadtxt('unprocessed_data/covtype.data', delimiter=',', dtype=np.int)  # Avoid tf.contrib since we want to get our hands dirty\n",
    "    print(loaded.shape)\n",
    "    all_ind = np.arange(0, len(loaded))\n",
    "    np.random.shuffle(all_ind)\n",
    "    train_ind = all_ind[: int(len(loaded) * 0.8)]\n",
    "    test_ind = all_ind[int(len(loaded) * 0.8): ]\n",
    "    \n",
    "    return loaded, all_ind, train_ind, test_ind\n",
    "\n",
    "\n",
    "load_data_run = False\n",
    "if load_data_run:\n",
    "    loaded, all_ind, train_ind, test_ind = load_data()\n",
    "else:\n",
    "    print('Flip load_data_run load in data from unprocessed_data folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip generate_run to generate_samples\n"
     ]
    }
   ],
   "source": [
    "def write(filename, indices, feature_proto, loaded):\n",
    "  # Round to the previous hour\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for i in tqdm.tqdm_notebook(indices):\n",
    "            datum = loaded[i, :]\n",
    "            feature = feature_proto.dataset_creation(datum)\n",
    "            example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example_proto.SerializeToString())\n",
    "\n",
    "def generate_samples(feature_proto, loaded, record_size=100000):\n",
    "    try:\n",
    "        os.mkdir('processed_data')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    time = str(datetime.datetime.now().replace(microsecond=0,second=0,minute=0)).replace(' ', '_')\n",
    "    for record_info in [('train', train_ind), ('test', test_ind)]:\n",
    "        mode = record_info[0]\n",
    "        indices = record_info[1]\n",
    "        if mode == 'test':\n",
    "            filename = 'processed_data/covtype_test_{}.tfrecord'.format(time)\n",
    "            write(filename, indices, feature_proto, loaded)\n",
    "        else:  # Training data\n",
    "            for i in tqdm.tqdm(range((len(record_info[1]) // record_size))):\n",
    "                filename = 'processed_data/covtype_train_{}_{}.tfrecord'.format(i, time)\n",
    "                start = i * record_size\n",
    "                end = (i + 1) * record_size\n",
    "                write(filename, indices[start:end], feature_proto, loaded)\n",
    "            write(filename, indices[end:], feature_proto, loaded)\n",
    "    # using your storage system -S3 or some other file hosting service\n",
    "    # add the export here\n",
    "        \n",
    "generate_run = False\n",
    "\n",
    "if generate_run:\n",
    "    generate_samples(feature_proto, loaded)\n",
    "else:\n",
    "    print('Flip generate_run to generate_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider\n",
    "\n",
    "Cell 1: Initialize the loader\n",
    "\n",
    "- even though there is the train, and test data in the tfRecordDataset, we pretend that they're two different runs of our pre-processor\n",
    "\n",
    "- The proto_wrap function is unnecessary here but for the sake of clarity I left it in. In the next tutorial, where I show you how to use the loaded data, we will remove it\n",
    "\n",
    "Cell 2: Provide\n",
    "\n",
    "- return an iterator that you can go through to iterate your dataset using either 1) stored tfrecords, or via tf.data.Datasets on numpy/ csvs\n",
    "\n",
    "Cell 3: Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def dataset_config(repeat=False, batch_size=32, num_cpus=None,\n",
    "                   # Used in tfRecordDatasets\n",
    "                   filenames: list=None, mapper=None, \n",
    "                   # Used in from_tensor_slices\n",
    "                  initializable:Tuple=False, sess=None, feed_dict=None,\n",
    "                  csv_filenames: list=None):\n",
    "    \"\"\"\n",
    "    Supports 3 modes: from tensor_slices, tfRecordDatasets and TextLineDataset (e.g csvs)\n",
    "    \"\"\"\n",
    "    tf_record = mapper is not None and filenames is not None\n",
    "    tensor_slices = initializable is not None and sess is not None and feed_dict is not None\n",
    "    use_csv = csv_filenames is not None\n",
    "    \n",
    "    if tf_record:\n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        dataset = dataset.map(mapper, num_parallel_calls=num_cpus)\n",
    "    elif tensor_slices:\n",
    "        assert initializable != False, 'initializable should be an iterable with placeholders'\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(initializable)\n",
    "    elif use_csv:\n",
    "        dataset = tf.data.TextLineDataset(csv_filenames)\n",
    "        dataset = dataset.map(mapper, num_parallel_calls=num_cpus)\n",
    "    else:\n",
    "        raise ValueError('If loading from tfRecordDatasets fill in filenames and mapper. '\n",
    "                        'If using from_tensor_slices feed in a initializable(placeholder iterable), session, and feed_dict')\n",
    "        \n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if tensor_slices:\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "    else:\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    next_element = iterator.get_next()\n",
    "    return next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_data/covtype_train_1_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_3_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_0_2019-01-06_02:00:00.tfrecord', 'processed_data/covtype_train_2_2019-01-06_02:00:00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "configuration = 'tf'  # Options: tf, csv, np\n",
    "\n",
    "# Option 1: reading tf.data.TFRecordDataset\n",
    "#     - requires that you generate it first\n",
    "if configuration == 'tf':\n",
    "    filename_list = []\n",
    "    for root, dirs, files in os.walk('processed_data/'):\n",
    "        for f in files:\n",
    "            if \"tfrecord\" in f and \"train\" in f:\n",
    "                filename_list.append(os.path.join(root, f))\n",
    "    print(filename_list)\n",
    "    num_cpus = os.cpu_count()\n",
    "    training_dataset_next = dataset_config(filenames=filename_list, mapper=feature_proto.unpack, num_cpus=num_cpus)\n",
    "\n",
    "    \n",
    "# Note, \n",
    "# Option 2: reading as a CSV\n",
    "# Note, in the later tutorials I will not include this in utils.py - I'm doing this mostly because \n",
    "# people kept asking for it\n",
    "elif configuration == 'csv':\n",
    "    filename_list = []\n",
    "    for root, dirs, files in os.walk('unprocessed_data/'):\n",
    "        for f in files:\n",
    "            if \".csv\" in f:\n",
    "                filename_list.append(os.path.join(root, f))\n",
    "    print(filename_list)\n",
    "    num_cpus = os.cpu_count()\n",
    "    \n",
    "    training_dataset_next = dataset_config(csv_filenames=filename_list, mapper=feature_proto.unpack_csv, num_cpus=num_cpus)\n",
    "\n",
    "\n",
    "# Option 3: Reading Np. Note that there is a 2GB limit and you should avoid this\n",
    "# I'm not actually going to implement this\n",
    "else:\n",
    "    with np.load('unprocessed_data/covtype.npy') as data:\n",
    "        features = data[\"features\"]\n",
    "        labels = data[\"labels\"]\n",
    "        \n",
    "        \n",
    "        features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "        labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "        \n",
    "        training_dataset_next = dataset_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect': array([290.,  99., 123.,  50.,  11., 297., 255., 114., 161., 143., 358.,\n",
      "        60., 135., 292., 164.,  28.,  15.,  78.,  21., 121.,  16., 324.,\n",
      "       319., 308., 307., 121., 164., 237., 144., 158.,  16., 197.],\n",
      "      dtype=float32),\n",
      " 'Elevation': array([2524., 3014., 2888., 2919., 2975., 3042., 2995., 2915., 3331.,\n",
      "       2975., 2952., 3235., 2942., 2903., 2692., 2783., 3816., 3007.,\n",
      "       2935., 2969., 3044., 2949., 2922., 3184., 2532., 2910., 2886.,\n",
      "       3227., 3251., 3272., 3293., 3193.], dtype=float32),\n",
      " 'Hillshade_3pm': array([176.,  85., 100., 135., 142., 169., 195., 131., 142., 114., 151.,\n",
      "        96., 153., 242., 149., 125., 139.,  49., 139., 135., 143., 185.,\n",
      "       199., 171., 207., 111., 146., 199., 133., 150., 138., 164.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_9am': array([202., 247., 247., 224., 205., 208., 190., 233., 231., 242., 191.,\n",
      "       231., 221.,  50., 227., 212., 208., 243., 212., 232., 211., 171.,\n",
      "       137., 204., 148., 243., 229., 188., 235., 225., 208., 218.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_Noon': array([239., 211., 224., 224., 212., 238., 248., 233., 243., 234., 206.,\n",
      "       201., 238., 179., 242., 205., 212., 179., 216., 234., 219., 217.,\n",
      "       202., 236., 218., 228., 243., 252., 238., 240., 211., 248.],\n",
      "      dtype=float32),\n",
      " 'Horizontal_Distance_To_Fire_Points': array([1898., 3972., 2227., 2021., 2081., 1301., 5459., 4355., 2980.,\n",
      "       1131., 1282., 2610., 4747.,  319., 3861., 2230., 2751., 2359.,\n",
      "       2742., 3208., 1075., 1434., 1536., 1507., 1235., 4066., 3427.,\n",
      "       1140., 1273., 2873.,  648., 1829.], dtype=float32),\n",
      " 'Horizontal_Distance_To_Hydrology': array([ 379.,  108.,   85.,  313.,  192.,  228.,  626.,  270.,  365.,\n",
      "        175.,  127.,  175.,  212.,  446.,   30.,  150., 1075.,  228.,\n",
      "         67.,   60.,   30.,  153.,  150.,   30.,  342.,  201.,  255.,\n",
      "        912.,  350.,  421.,  404.,  297.], dtype=float32),\n",
      " 'Horizontal_Distance_To_Roadways': array([ 210., 4169., 1530.,  918., 2368., 2184., 5769., 3376., 1641.,\n",
      "        977.,  607., 1559., 3838., 2812., 1712., 1437., 4662., 2398.,\n",
      "       1883., 2145., 4663., 3082.,  767., 5668.,  819., 3953., 3827.,\n",
      "       4053., 1604., 3416., 1116., 2093.], dtype=float32),\n",
      " 'Slope': array([ 6., 19., 17.,  7., 14.,  4., 13.,  8., 11., 17., 17., 18.,  1.,\n",
      "       48.,  7., 16., 13., 27., 11.,  7., 10., 18., 28.,  6., 24., 14.,\n",
      "       10., 17., 10.,  4., 14.,  9.], dtype=float32),\n",
      " 'Soil_Type': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
      " 'Vertical_Distance_To_Hydrology': array([ 41.,  29.,  20.,  91.,  12.,  20.,  99.,  57.,  66.,  38.,  22.,\n",
      "        46.,  24.,  -7.,   4.,  40., 407.,  90.,  -3.,  -5.,   0.,  33.,\n",
      "        71.,   4., 152.,  30.,  53., -42.,  61.,  96.,  39.,  31.],\n",
      "      dtype=float32),\n",
      " 'Wilderness_Area': array([3., 0., 2., 0., 2., 0., 0., 0., 0., 2., 2., 0., 0., 2., 0., 2., 2.,\n",
      "       2., 2., 2., 0., 2., 2., 0., 3., 0., 0., 2., 1., 0., 1., 2.],\n",
      "      dtype=float32)}\n",
      "{'Aspect': array([130.,  41., 132., 102., 342., 135., 349.,  90., 142., 352., 149.,\n",
      "        92., 180., 320., 280., 317., 100., 119., 194., 330., 105.,  82.,\n",
      "        18.,  16., 199., 144., 122.,  26.,  77.,  96.,  47.,  84.],\n",
      "      dtype=float32),\n",
      " 'Elevation': array([3392., 2581., 3273., 3075., 2705., 2877., 3404., 2939., 2795.,\n",
      "       2587., 3136., 3225., 2949., 2179., 2208., 2825., 2901., 2846.,\n",
      "       2947., 3144., 3209., 2683., 3255., 2906., 3179., 2554., 3081.,\n",
      "       3293., 2990., 3076., 3196., 2937.], dtype=float32),\n",
      " 'Hillshade_3pm': array([104., 122., 118.,  41., 168., 123., 160., 141., 125., 155., 149.,\n",
      "       103., 156., 191., 239., 196., 113.,  76., 161., 168., 113., 144.,\n",
      "       148., 142., 169., 132., 124., 140.,  75.,  87., 108.,  44.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_9am': array([246., 221., 241., 253., 174., 239., 202., 226., 239., 183., 225.,\n",
      "       241., 223., 163., 112., 155., 239., 252., 197., 202., 240., 224.,\n",
      "       215., 211., 214., 236., 238., 216., 241., 246., 224., 246.],\n",
      "      dtype=float32),\n",
      " 'Hillshade_Noon': array([227., 210., 232., 188., 205., 234., 224., 233., 237., 203., 239.,\n",
      "       217., 244., 216., 222., 213., 224., 213., 246., 231., 225., 233.,\n",
      "       226., 217., 252., 238., 232., 220., 196., 211., 202., 180.],\n",
      "      dtype=float32),\n",
      " 'Horizontal_Distance_To_Fire_Points': array([2917.,  663., 3700., 1273., 2107., 1321., 1556., 6333., 1765.,\n",
      "       1861., 1981., 1891., 5568.,  984.,  981.,  331., 1708., 2030.,\n",
      "       2228., 1260., 2591., 4732.,  175., 4082.,  664., 2365.,  890.,\n",
      "       2148., 1595.,  379., 1641., 1657.], dtype=float32),\n",
      " 'Horizontal_Distance_To_Hydrology': array([513., 190., 212., 175.,  67., 324., 684.,  90.,  30., 108., 277.,\n",
      "       484., 497.,  60.,  30.,  67.,  60., 485., 192.,  90.,  30.,   0.,\n",
      "        90.,  30., 108.,   0., 120., 700., 240.,  85., 497.,  30.],\n",
      "      dtype=float32),\n",
      " 'Horizontal_Distance_To_Roadways': array([1964., 2601., 2474., 2739., 2439., 1914., 1410., 3515., 1567.,\n",
      "        384., 1173., 1124., 5919., 1321., 1332., 4527., 1475., 1935.,\n",
      "       1833.,  664., 1673., 2442., 6319., 5284., 2160., 2771., 4336.,\n",
      "        875.,  474.,  912.,  488.,  514.], dtype=float32),\n",
      " 'Slope': array([17., 13., 14., 29., 20., 12., 10.,  4., 13., 19.,  4., 14.,  7.,\n",
      "       20., 33., 23., 12., 23., 32.,  8., 12.,  3.,  7., 11., 14., 11.,\n",
      "       10.,  9., 21., 18., 16., 28.], dtype=float32),\n",
      " 'Soil_Type': array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
      " 'Vertical_Distance_To_Hydrology': array([-72.,  59., -40.,  52.,  23., 102.,  65., -12.,   4.,  33.,  16.,\n",
      "       120.,  46.,  -3.,  26.,  26.,  -7., 145.,  60.,  -2.,  -4.,   0.,\n",
      "         3.,   3.,  -3.,   0.,   7., 155., -28.,  11.,  53.,  18.],\n",
      "      dtype=float32),\n",
      " 'Wilderness_Area': array([1., 2., 1., 2., 2., 2., 2., 0., 2., 0., 0., 0., 0., 3., 3., 0., 0.,\n",
      "       2., 2., 1., 0., 0., 0., 0., 2., 2., 0., 0., 0., 2., 0., 2.],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Lazy execution\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(2):\n",
    "        features, label = sess.run(training_dataset_next)\n",
    "        pprint.pprint(features)\n",
    "    \n",
    "# Eager execution\n",
    "#features, label = training_dataset_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done! \n",
    "\n",
    "And with that, we're done! We've \n",
    "\n",
    "1) taken a non-trivial dataset, \n",
    "2) converted it into a `tfRecord`\n",
    "3) shown how to unload it and read from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
